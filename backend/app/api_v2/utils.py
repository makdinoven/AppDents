import re
import csv
import json
import logging
from datetime import datetime
from io import StringIO
from typing import Dict, Any, Union, Tuple, Optional

from bs4 import BeautifulSoup
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from ..db.database import get_async_db
from ..models.models_v2 import Author, Landing

router = APIRouter()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ–ª–µ–π: –∫–ª—é—á ‚Äì –∏–º—è –≤ –¥–∞–º–ø–µ, –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äì –∏–º—è –≤ –º–æ–¥–µ–ª–∏
FIELD_MAPPING = {
    'page_name': 'page_name',
    'course_name': 'landing_name',
    'old_price': 'old_price',
    'new_price': 'new_price',
    'course_program': 'course_program',
    'lessons_info': 'lessons_info',
    'lecturers_info': 'lecturers_info',
    'linked_courses': 'linked_courses',
    'preview_photo': 'preview_photo'
}


def remove_html_tags(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç HTML-—Ç–µ–≥–∏ –∏ inline —Å—Ç–∏–ª–∏ –∏–∑ —Å—Ç—Ä–æ–∫–∏.
    """
    if not text:
        return text
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=' ', strip=True)


def clean_json_data(data: Union[Dict[str, Any], list, str]) -> Union[Dict[str, Any], list, str]:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ –æ—á–∏—â–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç HTML-—Ç–µ–≥–æ–≤.
    """
    if isinstance(data, dict):
        return {k: clean_json_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [clean_json_data(item) for item in data]
    elif isinstance(data, str):
        return remove_html_tags(data)
    else:
        return data


def parse_insert_line(statement: str) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –ø–æ–ª–Ω—ã–π INSERT –æ–ø–µ—Ä–∞—Ç–æ—Ä –∏–∑ –¥–∞–º–ø–∞.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —Å —Ñ–ª–∞–≥–æ–º DOTALL
    –∏ –º–æ–¥—É–ª—å csv –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–Ω–∞—á–µ–Ω–∏–π.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏, –≥–¥–µ –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Å–æ–≥–ª–∞—Å–Ω–æ FIELD_MAPPING.
    """
    try:
        # –ü—Ä–∏–º–µ—Ä –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:
        # INSERT INTO `landings` (`id`, `page_name`, `course_name`, ... )
        # VALUES (442, 'ninja-sinus-lift', 'Ninja Sinus Lift', ... );
        pattern = re.compile(r"INSERT INTO `landings`\s*\((.*?)\)\s*VALUES\s*\((.*?)\);", re.DOTALL)
        match = pattern.search(statement)
        if not match:
            return None

        fields_part = match.group(1)
        values_part = match.group(2)

        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –ø–æ –∑–∞–ø—è—Ç–æ–π
        fields = [f.strip().strip('`') for f in fields_part.split(',')]

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º csv –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è values, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—è—Ç—ã–µ –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö
        csv_reader = csv.reader(StringIO(values_part), delimiter=',', quotechar="'", escapechar='\\')
        row = next(csv_reader)
        values = [val.strip() for val in row]

        # –ï—Å–ª–∏ –ø–æ–ª–µ id –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Ç–æ –æ–Ω–æ –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è
        if 'id' in fields:
            idx = fields.index('id')
            del fields[idx]
            del values[idx]

        parsed = {}
        for i, field in enumerate(fields):
            if field not in FIELD_MAPPING:
                continue
            mapped_field = FIELD_MAPPING[field]
            value = values[i]
            # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å { –∏–ª–∏ [, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º JSON –∏ –ø—ã—Ç–∞–µ–º—Å—è –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å
            if value.startswith('{') or value.startswith('['):
                try:
                    json_value = json.loads(value)
                    cleaned_json = clean_json_data(json_value)
                    parsed[mapped_field] = cleaned_json
                except json.JSONDecodeError:
                    parsed[mapped_field] = remove_html_tags(value)
            else:
                parsed[mapped_field] = remove_html_tags(value)
        return parsed

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:\n{statement}\n–û—à–∏–±–∫–∞: {str(e)}")
        return None


async def get_or_create_author(db: AsyncSession, name: str, description: str) -> Tuple[Author, bool]:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä–∞ –ø–æ –∏–º–µ–Ω–∏, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äì —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤–æ–≥–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (author, created), –≥–¥–µ created True, –µ—Å–ª–∏ –∞–≤—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω.
    """
    try:
        result = await db.execute(select(Author).where(Author.name == name))
        author = result.scalars().first()
        if not author:
            author = Author(
                name=name,
                description=remove_html_tags(description),
                language='EN'
            )
            db.add(author)
            await db.commit()
            await db.refresh(author)
            logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∞–≤—Ç–æ—Ä: {name}")
            return (author, True)
        return (author, False)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏/—Å–æ–∑–¥–∞–Ω–∏–∏ –∞–≤—Ç–æ—Ä–∞ '{name}': {str(e)}")
        raise


@router.post("/import-dump")
async def import_dump(
        file: UploadFile = File(..., description="SQL dump file"),
        db: AsyncSession = Depends(get_async_db)
):
    """
    –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç SQL –¥–∞–º–ø –≤ –±–∞–∑—É –ø—Ä–æ–µ–∫—Ç–∞.

    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ INSERT:
      ‚Ä¢ –ü–∞—Ä—Å–∏—Ç—Å—è –æ–ø–µ—Ä–∞—Ç–æ—Ä (—Å —É—á—ë—Ç–æ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ—Å—Ç–∏)
      ‚Ä¢ –£–¥–∞–ª—è—é—Ç—Å—è HTML-—Ç–µ–≥–∏ –∏ inline —Å—Ç–∏–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π (–∞ —Ç–∞–∫–∂–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–Ω—É—Ç—Ä–∏ JSON)
      ‚Ä¢ –ü–æ–ª—è –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ –Ω—É–∂–Ω–æ–º—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, course_name ‚Üí landing_name)
      ‚Ä¢ –°–æ–∑–¥–∞—ë—Ç—Å—è –æ–±—ä–µ–∫—Ç Landing, –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ä–æ–∫–æ–≤ (–Ω–∞ –æ—Å–Ω–æ–≤–µ lessons_info)
      ‚Ä¢ –ò–∑ lecturers_info –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–≤—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–∑–¥–∞—é—Ç—Å—è (–∏–ª–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ –±–∞–∑—ã) –∏ —Å–≤—è–∑—ã–≤–∞—é—Ç—Å—è
      ‚Ä¢ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤—Å—Ç–∞–≤–∫–∞ –≤ –ë–î

    –í –æ—Ç–≤–µ—Ç–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
      ‚Äì –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ INSERT
      ‚Äì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ª–µ–Ω–¥–∏–Ω–≥–æ–≤
      ‚Äì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
      ‚Äì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –∫–æ–º–º–∏—Ç–∞
      ‚Äì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
      ‚Äì –û–±—â–µ–µ –≤—Ä–µ–º—è –∏–º–ø–æ—Ä—Ç–∞
    """
    total_statements = 0
    landings_inserted = 0
    failed_parse = 0
    commit_errors = 0
    new_authors_created = 0

    try:
        start_time = datetime.now()
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–º–ø–∞ –∏–∑ —Ñ–∞–π–ª–∞: {file.filename}")
        contents = await file.read()
        content_str = contents.decode('utf-8')

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º finditer –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –≤—Å–µ—Ö INSERT –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ (–º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ)
        pattern = re.compile(r"(INSERT INTO `landings`\s*\(.*?\)\s*VALUES\s*\(.*?\);)", re.DOTALL)
        statements = pattern.findall(content_str)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ INSERT: {len(statements)}")
        total_statements = len(statements)

        for stmt in statements:
            parsed_data = parse_insert_line(stmt)
            if not parsed_data:
                failed_parse += 1
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä:\n{stmt[:200]}...")
                continue

            logger.info("üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ INSERT")

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª–µ–Ω–¥–∏–Ω–≥–∞ (–±–µ–∑ lecturers_info)
            landing_data = {k: v for k, v in parsed_data.items() if k != 'lecturers_info'}

            lessons_info = parsed_data.get('lessons_info')
            if isinstance(lessons_info, (dict, list)):
                landing_data['lessons_count'] = str(len(lessons_info))
            else:
                landing_data['lessons_count'] = "0"

            landing_data['language'] = 'EN'
            landing_data.setdefault('duration', '')

            landing = Landing(**landing_data)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ lecturers_info
            lecturers = parsed_data.get('lecturers_info', {})
            for lecturer_key in ['lecturer1', 'lecturer2', 'lecturer3']:
                lecturer_data = lecturers.get(lecturer_key)
                if lecturer_data and lecturer_data.get('name'):
                    author, created = await get_or_create_author(
                        db,
                        lecturer_data.get('name'),
                        lecturer_data.get('description', '')
                    )
                    if created:
                        new_authors_created += 1
                    landing.authors.append(author)
                    logger.debug(f"üë§ –ü—Ä–∏–≤—è–∑–∞–Ω –∞–≤—Ç–æ—Ä: {author.name}")

            db.add(landing)
            try:
                await db.commit()
                await db.refresh(landing)
                landings_inserted += 1
                logger.info(f"‚úÖ –õ–µ–Ω–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω, ID: {landing.id}")
            except Exception as commit_error:
                commit_errors += 1
                await db.rollback()
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–º–∏—Ç–∞ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:\n{stmt[:200]}...\n–û—à–∏–±–∫–∞: {str(commit_error)}")
                continue

        elapsed = datetime.now() - start_time
        logger.info(f"üèÅ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {elapsed}")
        return {
            "status": "success",
            "message": "–î–∞–º–ø —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω",
            "time_elapsed": str(elapsed),
            "statements_found": total_statements,
            "landings_inserted": landings_inserted,
            "statements_failed_parse": failed_parse,
            "commit_errors": commit_errors,
            "new_authors_created": new_authors_created
        }

    except Exception as e:
        logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {str(e)}", exc_info=True)
        await db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        await file.close()
